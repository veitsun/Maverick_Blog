# ---

# 研究内容一：基于国产智算的GPU虚拟化研究

## 3.1 攻关内容

随着人工智能技术的不断发展，尤其是在大模型训练和推理领域，计算需求呈现出前所未有的增长。尤其是基于深度学习的大规模神经网络，其参数量的暴增对计算资源提出了更高的要求，导致算力需求呈现出指数级增长。这一趋势显著增加了数据中心的硬件负担，尤其是在GPU等加速硬件上的压力。同时，随着AI技术的逐步普及和多样化应用场景的出现，计算资源的碎片化问题愈加突出，单点集群的电力消耗急剧上升，效率低下的问题亟待解决。
面对这一挑战，国产GPU虽然在性能和生态适配性上逐步提升，但仍然受到国际技术壁垒的限制。高端GPU市场仍然被NVIDIA等国际巨头所主导，国内的技术水平和产业链成熟度相对较低。尤其是在一些关键的AI应用领域，如超大规模模型的训练、推理任务中，国产GPU往往无法满足需求，且其性能和生态系统兼容性存在较大的差距。为了突破这一瓶颈，虚拟化技术成为一种重要的解决方案。通过虚拟化技术，可以将计算资源进行池化和动态调度，降低对单卡算力的依赖，实现计算资源的高效整合。虚拟化的实现将极大地提升国产GPU的利用率，进而支撑大规模AI任务的高效训练与推理。
然而，国产GPU在资源利用率和生态兼容性方面仍然存在明显短板。目前，国内GPU生态中缺乏统一的虚拟化技术标准和规范，导致虚拟化技术的实施存在较大的差异。不同厂商的GPU在硬件架构、软件栈以及虚拟化支持等方面存在差异，跨平台任务调度效率低下，资源的共享与管理难度较大。不同GPU型号之间的互操作性差，使得在实际应用中，资源浪费严重，硬件资源无法得到充分利用。随着智算中心从传统的单点集群逐步向分布式架构转型，如何高效地协调跨机房、跨地域的算力资源成为一个紧迫问题。尽管我国在分布式计算网络（如基于波分复用技术的超百千米分布式智算网）方面取得了一定进展，但现有的国产GPU在互联带宽和时延优化方面仍然存在不足。这直接影响了跨区域、跨平台的计算协同效率，尤其是在大规模并行计算和数据密集型任务的处理中，时延和带宽问题依然是瓶颈。国际主流的AI训练框架，如PyTorch和TensorFlow等，已经在NVIDIA的GPU平台上进行了优化，支持丰富的软件生态和工具链。而国产GPU往往在这些框架的支持上存在较大不足，特别是在AI训练和推理的核心能力上。虚拟化技术能够通过构建中间层抽象，为现有的AI框架提供更好的兼容性，降低开发与部署成本，使得国产GPU能够更好地融入现有的AI生态。

虚拟化技术不仅是提升国产GPU资源利用率的关键突破口，也是解决国内GPU技术“卡脖子”问题的重要途径。通过建立统一的虚拟化技术标准、优化跨平台资源调度、提高分布式算力协同效率、加强AI框架兼容性等措施，虚拟化技术能够有效地弥补国产GPU在性能和生态方面的短板，为国产GPU的推广和应用铺平道路，并加速我国智能计算硬件自主可控的进程。

## 3.2 技术路线

围绕国产多元异构智算资源，实现资源高效利用，算力灵活供给，有两种设计思路。设备直通方案和API重定向方案。

设备直通方案：

客户端操作系统接收到GPU调用请求，通过PCIe中断，切换上下文环境到宿主机操作系统。加载GPU的SR-IOV（Single Root I/O Virtualization）驱动，创建四个虚拟设备，生成四个VF(Virtual Function) 。通过VFIO驱动框架（帮助管理和分配虚拟设备，可在安全的，受IOMMU保护的环境中直接访问对用户空间的直接访问），将生成的VF分配给不同的虚拟机，让虚拟机直接访问虚拟设备。

这种方案实际上是由客户操作系统使用原生驱动和硬件，支持PCI/PCIe设备直通，提供接近原生的GPU性能。缺点是这种方案缺少必要的中间层来跟踪和维护GPU状态，不支持不同虚拟机之间进行数据实时迁移的虚拟机高级特性。且只能根据国产GPU芯片一分四的特性，最多支持四个虚拟机，灵活性低。

API重定向方案：

这种方案主要是在库的层级上进行虚拟化。这种方法通过拦截客户操作系统中的GPU调用，并将其转发到宿主机或者远程机器上的GPU进行处理。拦截的GPU调用请求通过共享内存的方式或者网络通信的方式将请求重定向到宿主机的操作系统中。重定向的请求被宿主机处理，只有结果通过包装库传递给应用程序。这种重定向方法可以模拟GPU执行环境，不需要在客户机的操作系统中暴露物理GPU设备。

为了实现这种方案我们需要4个关键模块，分别是包装库，前端驱动，后端驱动和通信模块。

1）包装库是一个与原生GPU库具有相同API，安装在客户机操作系统中，是为了在GPU调用请求到达虚拟机上客户操作系统中的GPU驱动之前从应用程序中拦截GPU API调用，并将截取的请求交付给前端驱动。

2）前端驱动和后端驱动分别放置在客户机操作系统和宿主机操作系统中，前端驱动将接受到的GPU调用请求的接口和参数进行封装和编码,打包成可传输的消息（序列化的二进制数据流），将序列化的二进制编码通过通信模块传递到后端驱动。前端驱动也需要接受后端驱动返回的计算结果，将其传递给客户机应用程序上，模拟本地GPU的响应。

3）后端驱动将接收到的二进制编码进行解析，还原为原始的API调用，将解析出的调用和参数传递给物理GPU驱动进行计算，并将计算结果（如渲染后的图像或计算数据）编码，将编码通过通信模块传递回前端驱动。后端驱动还需要将自身可以正常使用的GPU设备信息注册到管理模块中。在面对前端驱动传递过来的请求，为每个请求分配独立的服务线程，后端驱动统一管理本地GPU资源，按照一定的策略提供GPU资源，并将由于API调用修改的相关软硬件状态更新到管理器中GPU状态信息表。

4）通信模块是为了实现虚拟机和宿主机之间进行通信，选择通信策略，为虚拟化提供更高层语义的支持，如果宿主机和虚拟机在同一个物理机上，那么可以通过共享内存或者虚拟化专用通道进行数据交互，如果是进行云计算，物理GPU在远程的话，那么可以通过TCP/IP RPC等网络通信方式进行数据的交互。

这种GPU虚拟化方案通过高效的资源隔离、灵活的负载均衡、远程支持和透明的兼容性，确保了GPU资源的最大化利用、可靠性和安全性，尤其适用于云计算和多租户环境。

## 3.3 创新性

对于研究路线中的两种方案，其创新性主要表现在以下几点：

一， 库级虚拟化与API重定向：

与传统的设备直通方案不同，API重定向方案通过在库层拦截和重定向GPU调用，而非直接映射硬件设备。这种方法避免了直接暴露物理GPU设备，提高了安全性并增强了灵活性，尤其适用于多租户和远程GPU共享场景。创新性提出五层协同架构，包装库实现API拦截与接口透明化，前后端驱动构建分布式处理流水线，通信模块实现跨环境协议自适应，管理模块实现资源状态全局视图。各模块形成闭环反馈系统，突破传统API转发

二，跨物理机GPU支持：

通过通信模块支持共享内存或网络通信（如TCP/IP RPC），即便GPU位于远程物理机上，也能无缝实现虚拟化。这个特点使得GPU资源不仅局限于本地服务器，而是可以跨数据中心、跨地域进行分配和共享，满足云计算需求。本地环境采用“共享内存+虚拟化专用通道”双通道架构，远程环境构建TCP/IP RPC与RDMA混合传输协议。通信模块支持协议自动识别与QoS动态调整。

三，透明兼容性：

包装库的使用，使得原生GPU API和虚拟化后环境之间无缝衔接，不需要对现有应用进行修改，降低了开发和部署的复杂度，同时提高了GPU虚拟化的普适性。

四，高效的资源隔离与可扩展性：

与设备直通方案的固定虚拟机限制不同，这种API重定向方案通过软件层的重定向，提供了更高的灵活性和可扩展性，不受物理GPU硬件限制，可以实现更复杂的资源调度和虚拟机动态迁移。

通过多层次协同创新，在国产化约束条件下构建了兼具高性能与灵活性的智算资源调度体系，其创新性主要体现在异构资源整合架构、动态资源管理机制、跨层级通信协议设计等方面，解决了传统虚拟化方案在国产GPU环境下面临的性能损耗与资源僵化问题，为国产智算平台的弹性扩展提供了新的技术路径。

# 研究内容二：基于GPU池化的资源管理系统

## 3.1 攻关内容

随着国内高性能计算应用的迅速发展，图形处理单元（GPU）已成为执行大量并行计算任务的核心硬件。相比传统CPU，GPU在处理图像、视频和物理模拟等任务时，展现出极为优越的计算性能，尤其在机器学习、数据挖掘和图像识别等领域，GPU的加速作用不可忽视。

然而，GPU在资源分配与管理上仍面临挑战。尽管单个GPU具有强大的计算能力，但当多个GPU协同工作时，如何高效、灵活地管理这些资源，确保计算任务的并行性和最大化资源利用，依然是一个亟待解决的难题。

“GPU池化”技术通过将多个GPU资源结合成一个池（Pool），并采用动态管理机制，根据任务需求灵活调度资源，从而提升资源利用率和系统计算能力。尽管如此，在高负载和复杂任务场景中，如何智能地进行资源调度与优化，依然是技术实现的关键挑战。

动态管理技术能够根据任务的计算需求实时调整GPU资源分配，避免资源浪费，提升计算效率。在需要大量并行计算的任务中，如何最大化释放GPU的计算潜力，是优化系统性能的关键。传统的固定资源分配模式往往导致GPU资源的闲置或过载，造成计算性能的不均衡。而GPU池化的动态管理通过实时监控与智能调度，能够有效解决资源浪费问题，降低瓶颈并确保任务的高效执行。

随着深度学习模型规模的不断扩展，单个GPU的计算能力已难以满足需求。GPU池化技术通过多个GPU的协同工作，突破了这一限制，为大规模并行计算任务提供了强有力的支持，推动了计算密集型领域的发展。此外，计算需求的多样性要求系统具备灵活的资源调度能力。某些任务可能仅需少量GPU资源，而另一些则需要大规模GPU支持。基于GPU池化的动态管理技术能够根据实际需求快速调整资源分配，提升系统的灵活性与适应性。

高效的GPU资源管理不仅对深度学习领域至关重要，也能推动大规模AI应用的普及，尤其是在图像处理、自然语言处理和智能推荐等领域的广泛应用。

## 3.2 技术路线

在基于GPU池化的资源管理系统中，核心目标是实现GPU资源的高效管理和利用。为此，可以设计一个智能化的管理模块，来实现对GPU资源的动态调度、负载均衡以及故障恢复等功能。具体的技术实现可以分为以下几个关键部分：

1）GPU状态信息维护：管理模块首先需要维护一个**GPU状态信息表**，记录每个GPU的当前状态、使用情况、负载程度以及可用性。这个表格将实时更新GPU的使用状况，包括计算能力、存储空间和内存利用率等，以便为后续的资源调度提供准确的数据支持。

2）GPU资源隔离与划分：为了高效管理GPU资源，系统将GPU的算力进行隔离和划分。这种划分可以基于不同的任务需求，例如按计算能力、内存大小或工作负载对GPU进行分片。每个GPU资源将被切分成多个**切片**（Slices），每个切片代表一个虚拟化的GPU资源单元，专门为某个特定任务或用户服务。

3）动态调度：动态调度是基于GPU池化资源管理的核心。管理模块通过高级调度算法（如最短作业优先、资源需求预测、负载均衡策略等），根据用户任务的需求，实时调度GPU资源。当GPU资源长时间处于空闲状态时，系统会自动回收这些资源，将其重新分配给新的任务。这种回收机制基于空闲时间阈值，当某个GPU的空闲时间超过预设标准时，资源将被标记为可回收资源，系统会将其调度到其他高需求任务中。调度算法根据GPU的当前负载、任务优先级、计算需求等因素，智能选择最合适的GPU资源。在资源池内的GPU之间，算法会自动优化计算负载分配，避免某些GPU超负荷运行，而其他GPU资源处于闲置状态。

4）负载均衡：在GPU池化资源的动态管理中，**负载均衡**是一个至关重要的环节。负载均衡通过实时监控各个GPU的负载情况，当某个切片的GPU计算压力过大时，系统会自动进行负载迁移，将部分计算任务转移到其他GPU上，从而避免计算瓶颈的发生。当检测到某个GPU的负载超过设定阈值时，系统会通过调度机制将计算任务迁移至负载较轻的GPU。此时，动态调度算法会根据任务的计算特性和GPU的计算能力，确保资源的最优分配，避免某些GPU因过载而影响计算性能。在多GPU资源池中，可能存在不同计算能力的GPU。负载均衡机制考虑到这一点，能够根据GPU的计算能力和任务要求，进行异构资源的高效调度，将任务分配给最合适的GPU，确保任务执行的最大效率。

5）故障恢复：当某个GPU切片发生故障时，系统需要迅速将受影响的任务转移到其他可用GPU上，以保证计算任务的连续性和稳定性。系统通过实时监控GPU的健康状态，快速检测GPU故障。一旦发现某个GPU切片出现故障，管理模块会立即启动故障恢复机制，将该GPU上的任务迁移到其他可用的GPU资源上，确保任务不中断。故障恢复的核心在于任务的平滑迁移。管理模块会选择负载较低的GPU资源，并通过虚拟化技术实现任务的动态迁移，确保计算任务的快速恢复和最小的性能损失。

通过引入动态管理技术，基于GPU池化的资源管理系统能够高效地调度和优化计算资源，实现负载均衡、故障恢复、资源回收等功能。这一技术实现不仅适用于高性能计算和深度学习等计算密集型任务，也为未来的大规模并行计算任务提供了更加灵活、智能的资源管理方案。

## 3.3 创新性

该项技术通过动态调度、负载均衡、故障恢复等机制，对GPU资源进行智能管理和优化，确保不同虚拟机之间的资源共享不产生冲突，并能够根据计算压力进行自动调整，提升了资源的利用效率和系统的稳定性。本系统创新性地提出了“状态信息表+软硬件协同监控”的架构，GPU状态信息表不仅实时记录每个GPU的硬件配置、内存占用、计算负载等元数据，还与硬件监控系统深度融合，形成闭环反馈机制。软硬件协同监控能够实时捕捉GPU资源的变化，包括温度、性能波动、健康状态等关键数据，使得资源调度更加精准与高效。这种机制为后续的动态调度、故障恢复及负载均衡提供了强有力的支撑。通过创新性地将资源回收阈值与调度策略解耦，打破了传统系统中固定资源回收策略的局限性。动态调度算法不再仅依赖于固定的回收规则，而是基于任务负载和资源需求的实时变化，动态调整回收阈值，使得资源回收和调度更为灵活。通过精确的负载预测和任务调度，该系统能够智能选择最合适的GPU资源，确保GPU资源的高效利用与任务的优先级合理分配。在故障恢复方面，系统创新性地引入了GPU热迁移与状态快照重建机制。当某个GPU出现故障时，系统能够通过热迁移技术将任务迅速转移至其他健康的GPU，而不需要系统重启或长时间停机。此外，GPU的状态快照重建功能允许任务在故障发生时迅速恢复，保留GPU上任务的中间计算结果，减少计算任务的丢失或重复，确保任务的连续性和系统的高可用性。系统引入了基于计算压力预测模型的负载均衡策略。通过对GPU负载的实时监控和任务计算需求的预测，系统能够在任务执行前进行预调度，自动识别计算压力大的GPU切片，并提前进行负载迁移。这一预测模型能够根据任务特征和历史数据动态调整GPU资源分配，有效避免了负载过度集中于某些GPU的情况，从而优化整体计算效率和系统稳定性。系统支持对不同计算能力的GPU进行高效的异构资源调度。负载均衡算法考虑GPU的异构特性，能够根据每个GPU的计算能力、内存容量等因素，为不同类型的任务选择最合适的GPU切片进行处理。该创新使得GPU池化资源的使用更加灵活，能够在不同硬件资源条件下实现高效调度与最大化性能。

基于GPU池化的动态管理技术正通过算法、系统软件与硬件的协同创新，推动算力资源从“独占”向“共享”转型，其核心价值在于最大化资源利用率、降低企业成本，并为国产化替代与可持续发展提供技术基础。
