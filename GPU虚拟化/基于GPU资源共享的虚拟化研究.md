# 基于国产智算的GPU虚拟化研究

## 3.1 攻关内容

随着人工智能技术的不断发展，尤其是在大模型训练和推理领域，计算需求呈现出前所未有的增长。尤其是基于深度学习的大规模神经网络，其参数量的暴增对计算资源提出了更高的要求，导致算力需求呈现出指数级增长。这一趋势显著增加了数据中心的硬件负担，尤其是在GPU等加速硬件上的压力。同时，随着AI技术的逐步普及和多样化应用场景的出现，计算资源的碎片化问题愈加突出，单点集群的电力消耗急剧上升，效率低下的问题亟待解决。
面对这一挑战，国产GPU虽然在性能和生态适配性上逐步提升，但仍然受到国际技术壁垒的限制。高端GPU市场仍然被NVIDIA等国际巨头所主导，国内的技术水平和产业链成熟度相对较低。尤其是在一些关键的AI应用领域，如超大规模模型的训练、推理任务中，国产GPU往往无法满足需求，且其性能和生态系统兼容性存在较大的差距。为了突破这一瓶颈，虚拟化技术成为一种重要的解决方案。通过虚拟化技术，可以将计算资源进行池化和动态调度，降低对单卡算力的依赖，实现计算资源的高效整合。虚拟化的实现将极大地提升国产GPU的利用率，进而支撑大规模AI任务的高效训练与推理。
然而，国产GPU在资源利用率和生态兼容性方面仍然存在明显短板。目前，国内GPU生态中缺乏统一的虚拟化技术标准和规范，导致虚拟化技术的实施存在较大的差异。不同厂商的GPU在硬件架构、软件栈以及虚拟化支持等方面存在差异，跨平台任务调度效率低下，资源的共享与管理难度较大。不同GPU型号之间的互操作性差，使得在实际应用中，资源浪费严重，硬件资源无法得到充分利用。随着智算中心从传统的单点集群逐步向分布式架构转型，如何高效地协调跨机房、跨地域的算力资源成为一个紧迫问题。尽管我国在分布式计算网络（如基于波分复用技术的超百千米分布式智算网）方面取得了一定进展，但现有的国产GPU在互联带宽和时延优化方面仍然存在不足。这直接影响了跨区域、跨平台的计算协同效率，尤其是在大规模并行计算和数据密集型任务的处理中，时延和带宽问题依然是瓶颈。国际主流的AI训练框架，如PyTorch和TensorFlow等，已经在NVIDIA的GPU平台上进行了优化，支持丰富的软件生态和工具链。而国产GPU往往在这些框架的支持上存在较大不足，特别是在AI训练和推理的核心能力上。虚拟化技术能够通过构建中间层抽象，为现有的AI框架提供更好的兼容性，降低开发与部署成本，使得国产GPU能够更好地融入现有的AI生态。

虚拟化技术不仅是提升国产GPU资源利用率的关键突破口，也是解决国内GPU技术“卡脖子”问题的重要途径。通过建立统一的虚拟化技术标准、优化跨平台资源调度、提高分布式算力协同效率、加强AI框架兼容性等措施，虚拟化技术能够有效地弥补国产GPU在性能和生态方面的短板，为国产GPU的推广和应用铺平道路，并加速我国智能计算硬件自主可控的进程。

## 3.2 技术路线

围绕国产多元异构智算资源，实现资源高效利用，算力灵活供给，有两种设计思路。设备直通方案和API重定向方案。

设备直通方案：

客户端操作系统接收到GPU调用请求，通过PCIe中断，切换上下文环境到宿主机操作系统。加载GPU的SR-IOV（Single Root I/O Virtualization）驱动，创建四个虚拟设备，生成四个VF(Virtual Function) 。通过VFIO驱动框架（帮助管理和分配虚拟设备，可在安全的，受IOMMU保护的环境中直接访问对用户空间的直接访问），将生成的VF分配给不同的虚拟机，让虚拟机直接访问虚拟设备。

这种方案实际上是由客户操作系统使用原生驱动和硬件，支持PCI/PCIe设备直通，提供接近原生的GPU性能。缺点是这种方案缺少必要的中间层来跟踪和维护GPU状态，不支持不同虚拟机之间进行数据实时迁移的虚拟机高级特性。且只能根据国产GPU芯片一分四的特性，最多支持四个虚拟机，灵活性低。

API重定向方案：

这种方案主要是在库的层级上进行虚拟化。这种方法通过拦截客户操作系统中的GPU调用，并将其转发到宿主机或者远程机器上的GPU进行处理。拦截的GPU调用请求通过共享内存的方式或者网络通信的方式将请求重定向到宿主机的操作系统中。重定向的请求被宿主机处理，只有结果通过包装库传递给应用程序。这种重定向方法可以模拟GPU执行环境，不需要在客户机的操作系统中暴露物理GPU设备。

为了实现这种方案我们需要5个关键模块，分别是包装库，前端驱动，后端驱动，通信模块，管理模块。

1）包装库是一个与原生GPU库具有相同API，安装在客户机操作系统中，是为了在GPU调用请求到达虚拟机上客户操作系统中的GPU驱动之前从应用程序中拦截GPU API调用，并将截取的请求交付给前端驱动。

2）前端驱动和后端驱动分别放置在客户机操作系统和宿主机操作系统中，前端驱动将接受到的GPU调用请求的接口和参数进行封装和编码,打包成可传输的消息（序列化的二进制数据流），将序列化的二进制编码通过通信模块传递到后端驱动。前端驱动也需要接受后端驱动返回的计算结果，将其传递给客户机应用程序上，模拟本地GPU的响应。

3）后端驱动将接收到的二进制编码进行解析，还原为原始的API调用，将解析出的调用和参数传递给物理GPU驱动进行计算，并将计算结果（如渲染后的图像或计算数据）编码，将编码通过通信模块传递回前端驱动。后端驱动还需要将自身可以正常使用的GPU设备信息注册到管理模块中。在面对前端驱动传递过来的请求，为每个请求分配独立的服务线程，后端驱动统一管理本地GPU资源，按照一定的策略提供GPU资源，并将由于API调用修改的相关软硬件状态更新到管理器中GPU状态信息表。

4）通信模块是为了实现虚拟机和宿主机之间进行通信，选择通信策略，为虚拟化提供更高层语义的支持，如果宿主机和虚拟机在同一个物理机上，那么可以通过共享内存或者虚拟化专用通道进行数据交互，如果是进行云计算，物理GPU在远程的话，那么可以通过TCP/IP RPC等网络通信方式进行数据的交互。

5）管理模块维护GPU的状态信息，在管理端维护一个GPU状态信息表。将GPU算力资源进行隔离，划分，调度。在管理模块实现对GPU资源的统一管理，包括动态调度，负载均衡和故障恢复。动态调度根据用户所占的资源空闲时间到达一定程度时，进行资源回收，依据高级调度算法进行资源调度。负载均衡可以在某个切片GPU计算压力过大的时候，调整计算负载，通过动态调度来选择合适的GPU资源来分散局部压力。故障恢复可以在某个切片GPU出现故障的时候，将任务重新转移到其他的可用GPU资源上面。

这种GPU虚拟化方案通过高效的资源隔离、灵活的负载均衡、远程支持和透明的兼容性，确保了GPU资源的最大化利用、可靠性和安全性，尤其适用于云计算和多租户环境。

## 3.3 创新性

对于研究路线中的两种方案，其创新性主要表现在以下几点：

一， 库级虚拟化与API重定向：

与传统的设备直通方案不同，API重定向方案通过在库层拦截和重定向GPU调用，而非直接映射硬件设备。这种方法避免了直接暴露物理GPU设备，提高了安全性并增强了灵活性，尤其适用于多租户和远程GPU共享场景。创新性提出五层协同架构，包装库实现API拦截与接口透明化，前后端驱动构建分布式处理流水线，通信模块实现跨环境协议自适应，管理模块实现资源状态全局视图。各模块形成闭环反馈系统，突破传统API转发

二，跨物理机GPU支持：

通过通信模块支持共享内存或网络通信（如TCP/IP RPC），即便GPU位于远程物理机上，也能无缝实现虚拟化。这个特点使得GPU资源不仅局限于本地服务器，而是可以跨数据中心、跨地域进行分配和共享，满足云计算需求。本地环境采用“共享内存+虚拟化专用通道”双通道架构，远程环境构建TCP/IP RPC与RDMA混合传输协议。通信模块支持协议自动识别与QoS动态调整。

三，动态资源管理与负载均衡：

管理模块的设计通过动态调度、负载均衡、故障恢复等机制，对GPU资源进行智能管理和优化，确保不同虚拟机之间的资源共享不产生冲突，并能够根据计算压力进行自动调整，提升了资源的利用效率和系统的稳定性。创新性的提出“状态信息表+软硬件协同监控”机制，GPU状态信息表实时记录硬件配置，内存占用，计算负载等元数据。动态调度算法实现资源回收阈值与调度策略解耦。故障恢复系统支持GPU热迁移与状态快照重建。负载均衡引入计算压力预测模型，支持预调度。

四，透明兼容性：

包装库的使用，使得原生GPU API和虚拟化后环境之间无缝衔接，不需要对现有应用进行修改，降低了开发和部署的复杂度，同时提高了GPU虚拟化的普适性。

五，高效的资源隔离与可扩展性：

与设备直通方案的固定虚拟机限制不同，这种API重定向方案通过软件层的重定向，提供了更高的灵活性和可扩展性，不受物理GPU硬件限制，可以实现更复杂的资源调度和虚拟机动态迁移。

通过多层次协同创新，在国产化约束条件下构建了兼具高性能与灵活性的智算资源调度体系，其创新性主要体现在异构资源整合架构、动态资源管理机制、跨层级通信协议设计等方面，解决了传统虚拟化方案在国产GPU环境下面临的性能损耗与资源僵化问题，为国产智算平台的弹性扩展提供了新的技术路径。
