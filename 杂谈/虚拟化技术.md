**研究方向：**

资源调度和分配听起来很重要，因为如何高效地将GPU资源分配给多个用户或任务直接影响整体性能。比如动态分配策略，根据负载调整资源，可能需要机器学习的方法来预测资源需求。但动态调整可能会引入延迟，如何平衡效率和延迟是个问题。

虚拟化开销优化方面，现有的虚拟化技术通常有一定的性能损失，尤其是在数据传输和上下文切换时。如何减少这些开销，比如改进前后端驱动，或者使用硬件辅助技术，可能值得研究。比如Intel的GVT-g和NVIDIA的硬件虚拟化支持，是否可以通过硬件和软件协同设计来降低开销？

多租户隔离是关键，特别是在公有云中，确保一个用户的任务不会影响其他用户。现有的隔离机制可能在内存、计算单元或缓存层次上有漏洞，需要更细粒度的控制。比如研究如何在GPU内部实现更强的隔离，或者检测和防止侧信道攻击。

容器化支持方面，Kubernetes等平台需要更细粒度的GPU资源管理，比如共享GPU内存和计算核心。现有的容器运行时可能不支持动态分配，如何与虚拟化技术结合，实现更灵活的资源管理。

**研究潜力：**

GPU资源池化最理想的方案是屏蔽底层GPU异构资源细节(支持英伟达和国产各厂商GPU) ，分离上层AI 框架应用和底层GPU类型的耦合性。

不过目前AI框架和GPU类型是紧耦合的，尚没有实现的方案抽象出一层能屏蔽异构GPU。基于不同框架开发的应用在迁移到其他类型GPU时，不得不重新构建应用，至少得迁移应用到另外的GPU，往往需要重新的适配和调试。

算力隔离、故障隔离是GPU虚拟化和池化的关键。算力隔离有硬件隔离也就是空分的方式，MPS共享CUDA Context方式和Time Sharing时分的方式。越靠底层，隔离效果越好，如MIG硬件算力隔离方案，是一种硬件资源隔离、故障隔离方式，效果最好。但硬件设备编程接口和驱动接口往往是不公开的，所以对厂商依赖大，实施的难度非常大，灵活性差，如支持Ampere架构的A100等，最多只能切分为7个MIG实例等。

NVIDIA MPS是除MIG外，算力隔离最好的。它将多个CUDA Context合并到一个CUDA Context中，省去Context Switch的开销并在Context内部实现了算力隔离，但也致额外的故障传播。MIG和MPS优缺点都非常明显，实际工程中用的并不普遍。采用API转发的多任务GPU时间分片的实现模式相对容易实现和应用最广。

AI应用通过GPU调用层次，实现多层次资源池化，涵盖CUDA层、Driver层及硬件设备层。这些层次将加速应用精准转发至GPU资源池。底层转发性能损失小，操作空间大，但编程复杂度与难度也随之提升。精准配置，高效利用，助力AI性能飞跃。
