## 研究背景

LLM 大型语言模型

传统的神经处理单元（NPU）在处理GEMM时表现出色，但在处理GEMV时效率较低。

而内存中处理（PIM）技术则擅长处理GEMV，但在GEMM方面的计算能力有限。

NeuPIMS 是一种新型的异构加速系统，专为批量推理大型语言模型而设计。NeuPIMs能够有效平衡内存带宽和计算资源的利用，从而提高LLM推理的吞吐量。

实验结果表明，与仅使用GPU、仅使用NPU以及传统的NPU+PIM集成加速方法相比，NeuPIMs在吞吐量上分别提高了3倍、2.4倍和1.6倍。

## 研究目标与内容

基于PIM和NPU的异构算子调度推理加速框架，构建一个NPU-PIM系统以提升大模型推理过程的整体效率。

- transformer 模型的算子级别任务调度优化

- PIM上的并发数据布局 和 内存利用率的提高

## 整体框架

### 整体框架概述

目的： 设计一个高效的异构算子调度系统

计算密集型任务优先在NPU上执行，而内存密集型任务则下推至PIM，减少数据在 NPU 和 PIM 之间频繁交换，并最大化整体硬件资源利用率，从而提升整体推理性能。

核心：算子级别调度

大模型推理过程分为两个阶段：

prefill 和 decode

- prefill 是用户输入完 prompt 到生成首个 token 的过程， decode 则为生成首个 token 到推理停止的过程。（prompt 是输入到语言模型的文本，用于引导模型生成特定类型的输出）

- decode 则为生成首个 token 到推理停止的过程。
